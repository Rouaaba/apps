cotaf is python
test cases also written in python
using test center and polarion

the flow is : a loop:
tets cases defined inside polarion then squish develop test case and elaboratet them because its just the squelette in polarion and once done thenyou can trigger it and orchestrate the test execution from jenkins or manually with the id, then have an upload of the results in test center (repo of the execution of test cases) then versionning within github for the dev platform and gitlab for validation platform, so one validated then same loop happens(jenkins-test-test center) and also the report can be uploaded in polarion

test case description: app team themselves create test cases for their own app based on their requirements

polarion once generated test case then squish takes over and takes the id of the test that becomes the name of the test so thats a naming convension

a team has their own dashboard mapped service called steer used to upload the results so not just test center but they need the capability of squish to produce test results

test cases composed of test steps

test center is not just for storage but can be also used to put comments so its like an itermediate layer between squish and the final polarion test results

teams working on app create and run their test cases and they need new features then tehy request to the test integration and automation team

they also have a dedicated team to write end to end tests on cotaf involving different applications
the application can be ditributed against different technologies (java, linux, windows) also web applications developped internally but can involve external suppliers like ifss
they want tool that can support linux also support for qt applications


the only tracebality they have is between test cases and requirements based on the id
tool needs to be easy to learn and use 

in test center they have a screenshot of the tets (but with new versionpf squish they dont use it)

for performance testing: they want have implemneted a library toi catch the response time and then those are logged into a file and then sent to elastic then dynatrace


they dont want the tool to take one sec for each action they do like click on button etc, so make a margin for the tool to process

the tool shouldnt use a huge amount of cpu or memry because it will slow down the app 

the data/metrics they need: the name of the action, testing this application on this client,  the response time and the time of the action


the tests they do: integration on ui level, system tests

test data on test data in global (log files or excel files)

bdd makes sense but their ezngineers already define requirements in the bdd style in polarion that gets translated into readable code

tests executed on skyguide infra but also on skyoff(?) infra











